{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8dd7df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "210ca08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prajakta B\\AppData\\Local\\Temp\\ipykernel_14520\\885378504.py:4: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_features = pd.read_csv(\"C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom.data\",\n",
      "C:\\Users\\Prajakta B\\AppData\\Local\\Temp\\ipykernel_14520\\885378504.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_labels = pd.read_csv(\"C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom_labels.data\",\n"
     ]
    }
   ],
   "source": [
    "#Read Data from soruces\n",
    "\n",
    "# Read the sensor feature file (590 columns, no header)\n",
    "df_features = pd.read_csv(\"C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom.data\",\n",
    "                          delim_whitespace=True,  # or sep=' '\n",
    "                          header=None)\n",
    "\n",
    "# Read the labels file (two columns: label and timestamp)\n",
    "df_labels = pd.read_csv(\"C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom_labels.data\",\n",
    "                        delim_whitespace=True,\n",
    "                        header=None)\n",
    "\n",
    "# df_features.head()\n",
    "# df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d08a3214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prajakta B\\AppData\\Local\\Temp\\ipykernel_14520\\4161458270.py:6: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  secom_data = pd.read_csv('C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom.data', delim_whitespace=True, header=None)\n",
      "C:\\Users\\Prajakta B\\AppData\\Local\\Temp\\ipykernel_14520\\4161458270.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels = pd.read_csv('C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom_labels.data', delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "#Combining data and creating proper structure of table\n",
    "\n",
    "\n",
    "# Loading dataset \n",
    "try:\n",
    "    secom_data = pd.read_csv('C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom.data', delim_whitespace=True, header=None)\n",
    "    labels = pd.read_csv('C:/Users/Prajakta B/Desktop/SECOM_DATA/secom/secom_labels.data', delim_whitespace=True, header=None)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please verify file paths and names.\")\n",
    "    exit()\n",
    "    \n",
    "# Rename columns like feature_1,2.. target_1,2..\n",
    "num_sensors = secom_data.shape[1]  # Number of sensor columns\n",
    "new_columns = [f'feature_{i+1}' for i in range(num_sensors)] \n",
    "secom_data.columns = new_columns\n",
    "\n",
    "labels.columns = [\"is_faulty\", \"date_time\"]\n",
    "\n",
    "df = pd.concat([\n",
    "          secom_data, labels\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e531705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency distribution BEFORE sampling:\n",
      "           proportion\n",
      "is_faulty            \n",
      "-1           0.933631\n",
      " 1           0.066369\n",
      "           count\n",
      "is_faulty       \n",
      "-1          1463\n",
      " 1           104\n",
      "----------------------------------------\n",
      "Frequency distribution in TRAIN set:\n",
      "           proportion\n",
      "is_faulty            \n",
      "-1            0.93327\n",
      " 1            0.06673\n",
      "           count\n",
      "is_faulty       \n",
      "-1           979\n",
      " 1            70\n",
      "----------------------------------------\n",
      "Frequency distribution in TEST set:\n",
      "           proportion\n",
      "is_faulty            \n",
      "-1           0.934363\n",
      " 1           0.065637\n",
      "           count\n",
      "is_faulty       \n",
      "-1           484\n",
      " 1            34\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test & train dataset split with stratified sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_colms = [col for col in df.columns if col not in ['is_faulty']]\n",
    "# Features: all columns starting with 'feature_'\n",
    "X = df[feature_colms]\n",
    "\n",
    "# Target: 'is_faulty'\n",
    "y = df['is_faulty']\n",
    "\n",
    "# Show frequency distribution before sampling\n",
    "print(\"Frequency distribution BEFORE sampling:\")\n",
    "print(y.value_counts(normalize=True).rename('proportion').to_frame())\n",
    "print(y.value_counts().rename('count').to_frame())\n",
    "print('-' * 40)\n",
    "\n",
    "# Stratified train-test split (67% train, 33% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.33,        # 33% test, 67% train\n",
    "    random_state=42,       # seed value\n",
    "    stratify=y             # Stratified split based on is_faulty\n",
    ")\n",
    "\n",
    "\n",
    "# Show frequency distribution in train set\n",
    "print(\"Frequency distribution in TRAIN set:\")\n",
    "print(y_train.value_counts(normalize=True).rename('proportion').to_frame())\n",
    "print(y_train.value_counts().rename('count').to_frame())\n",
    "print('-' * 40)\n",
    "\n",
    "# Show frequency distribution in test set\n",
    "print(\"Frequency distribution in TEST set:\")\n",
    "print(y_test.value_counts(normalize=True).rename('proportion').to_frame())\n",
    "print(y_test.value_counts().rename('count').to_frame())\n",
    "print('-' * 40)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_df_clean = pd.concat([X_train, y_train], axis=1)\n",
    "# Display datasets\n",
    "# train_df\n",
    "# test_df \n",
    "# train_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea63c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial number of columns: 592\n",
      "Number of zero-volatility columns removed: 116\n",
      "Final number of columns: 476\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning in train dataset train_df_clean\n",
    "# zero volatality columns\n",
    "\n",
    "# Count initial columns (including target)\n",
    "initial_col_count = train_df_clean.shape[1]\n",
    "print(f\"\\nInitial number of columns: {initial_col_count}\")\n",
    "\n",
    "# Identify feature columns (exclude target)\n",
    "feature_cols = [col for col in df.columns if col not in ['is_faulty', 'date_time']]\n",
    "\n",
    "# Find zero-volatility columns (std = 0)\n",
    "stds = train_df_clean[feature_cols].std()\n",
    "zero_vol_cols = stds[stds == 0].index.tolist()\n",
    "\n",
    "# Remove zero-volatility columns\n",
    "train_df_clean = train_df_clean.drop(columns=zero_vol_cols)\n",
    "\n",
    "# Count final columns (including target)\n",
    "final_col_count = train_df_clean.shape[1]\n",
    "print(f\"Number of zero-volatility columns removed: {len(zero_vol_cols)}\")\n",
    "print(f\"Final number of columns: {final_col_count}\")\n",
    "# print(train_df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9fadd52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns before removal: 476\n",
      "Number of columns with >50% missing values: 24\n",
      "Columns to drop due to missing values >50%:\n",
      "['feature_86', 'feature_110', 'feature_111', 'feature_112', 'feature_158', 'feature_159', 'feature_221', 'feature_245', 'feature_246', 'feature_247', 'feature_293', 'feature_294', 'feature_359', 'feature_383', 'feature_384', 'feature_385', 'feature_493', 'feature_517', 'feature_518', 'feature_519', 'feature_579', 'feature_580', 'feature_581', 'feature_582']\n",
      "Number of columns after removal: 452\n",
      "Number of columns removed: 24\n"
     ]
    }
   ],
   "source": [
    "# missing values column dropping\n",
    "# Count columns before removal\n",
    "before_cols = train_df_clean.shape[1]\n",
    "print(f\"Number of columns before removal: {before_cols}\")\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "missing_percent = train_df_clean.isnull().mean()\n",
    "\n",
    "# Identify columns with more than 50% missing values\n",
    "cols_to_drop = missing_percent[missing_percent > 0.50].index.tolist()\n",
    "\n",
    "print(f\"Number of columns with >50% missing values: {len(cols_to_drop)}\")\n",
    "print(\"Columns to drop due to missing values >50%:\")\n",
    "print(cols_to_drop)\n",
    "\n",
    "# Drop these columns\n",
    "train_df_clean = train_df_clean.drop(columns=cols_to_drop)\n",
    "\n",
    "# Count columns after removal\n",
    "after_cols = train_df_clean.shape[1]\n",
    "print(f\"Number of columns after removal: {after_cols}\")\n",
    "print(f\"Number of columns removed: {before_cols - after_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8f3fcf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature_1  feature_2  feature_3  feature_4  feature_5  feature_7  \\\n",
      "1263    2983.73    2459.87  2162.1333   998.9095     0.8826   104.9722   \n",
      "999     3045.11    2444.87  2215.1778  2192.1867     1.8829    85.6589   \n",
      "203     2981.95    2279.48  2205.2889  1630.3112     1.2733    98.8056   \n",
      "396     3127.07    2478.97  2198.7222  1534.2053     0.9374   104.1989   \n",
      "193     3024.15    2521.24  2191.7889  1133.3967     1.3808   106.6711   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "57      2935.94    2586.05  2164.4111  1206.6031     0.9799   100.5189   \n",
      "365     2988.92    2460.91  2178.0778   941.9524     0.8039   104.0167   \n",
      "1420    2975.74    2517.35  2162.5556  1041.0369     1.4305   100.4111   \n",
      "113     2928.16    2523.21  2210.6111  1184.6481     1.2577   102.9356   \n",
      "240     2900.69    2483.06  2191.1111  1564.6023     0.9366   102.5100   \n",
      "\n",
      "      feature_8  feature_9  feature_10  feature_11  ...  feature_583  \\\n",
      "1263     0.1246     1.3897     -0.0091     -0.0024  ...       0.5016   \n",
      "999      0.1237     1.4193      0.0113     -0.0007  ...       0.5008   \n",
      "203      0.1218     1.4334      0.0056      0.0012  ...       0.4965   \n",
      "396      0.1224     1.2476      0.0170      0.0125  ...       0.5043   \n",
      "193      0.1237     1.6035      0.0023      0.0114  ...       0.4936   \n",
      "...         ...        ...         ...         ...  ...          ...   \n",
      "57       0.1220     1.4786      0.0049      0.0116  ...       0.5048   \n",
      "365      0.1229     1.5829     -0.0278     -0.0324  ...       0.4976   \n",
      "1420     0.1238     1.4968     -0.0201     -0.0060  ...       0.4994   \n",
      "113      0.1201     1.4453     -0.0126      0.0152  ...       0.5016   \n",
      "240      0.1234     1.5192     -0.0168      0.0104  ...       0.5044   \n",
      "\n",
      "      feature_584  feature_585  feature_586  feature_587  feature_588  \\\n",
      "1263       0.0240       0.0053       4.7785       0.0152       0.0077   \n",
      "999        0.0190       0.0046       3.7891       0.0096       0.0191   \n",
      "203        0.0156       0.0038       3.1457       0.0123       0.0094   \n",
      "396        0.0089       0.0026       1.7695       0.0120       0.0104   \n",
      "193        0.0289       0.0065       5.8589       0.0112       0.0191   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "57         0.0138       0.0040       2.7309       0.0201       0.0220   \n",
      "365        0.0148       0.0032       2.9645       0.0291       0.0135   \n",
      "1420       0.0115       0.0033       2.3077       0.0299       0.0071   \n",
      "113        0.0160       0.0035       3.1882       0.0049       0.0144   \n",
      "240        0.0115       0.0029       2.2888       0.0154       0.0271   \n",
      "\n",
      "      feature_589  feature_590  is_faulty            date_time  \n",
      "1263       0.0029      50.5947         -1  03/10/2008 03:52:00  \n",
      "999        0.0069     197.5077         -1  21/09/2008 22:02:00  \n",
      "203        0.0026      76.4584         -1  15/08/2008 05:13:00  \n",
      "396        0.0036      86.7035         -1  22/08/2008 02:01:00  \n",
      "193        0.0058     170.4645         -1  11/08/2008 11:35:00  \n",
      "...           ...          ...        ...                  ...  \n",
      "57         0.0065     109.4273          1  30/07/2008 12:29:00  \n",
      "365        0.0045      46.4165         -1  21/08/2008 15:32:00  \n",
      "1420       0.0020      23.6431         -1  08/10/2008 10:37:00  \n",
      "113        0.0047     293.2614         -1  05/08/2008 06:21:00  \n",
      "240        0.0104     176.0329          1  17/08/2008 23:55:00  \n",
      "\n",
      "[1049 rows x 452 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Handle missing values with KNN\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Identify feature columns (exclude target)\n",
    "feature_cols = [col for col in train_df_clean.columns if col not in ['is_faulty', 'date_time']]\n",
    "\n",
    "# Select only numeric columns among features\n",
    "numeric_feature_cols = train_df_clean[feature_cols].select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Store 'is_faulty' and 'date_time' separately\n",
    "meta_cols = train_df_clean[['is_faulty', 'date_time']].copy()\n",
    "\n",
    "# Apply KNN imputer only to numeric feature columns\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputed_features = pd.DataFrame(\n",
    "    imputer.fit_transform(train_df_clean[numeric_feature_cols]),\n",
    "    columns=numeric_feature_cols,\n",
    "    index=train_df_clean.index\n",
    ")\n",
    "\n",
    "# If you have any non-numeric feature columns, add them back (if needed)\n",
    "non_numeric_cols = [col for col in feature_cols if col not in numeric_feature_cols]\n",
    "imputed_df = pd.concat([imputed_features, train_df_clean[non_numeric_cols]], axis=1)\n",
    "\n",
    "# Append 'is_faulty' and 'date_time' columns back\n",
    "train_df_clean = pd.concat([imputed_df, meta_cols], axis=1)\n",
    "\n",
    "print(train_df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a9c4eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped due to high correlation: 232\n",
      "\n",
      "List of dropped columns:\n",
      "['feature_575', 'feature_357', 'feature_387', 'feature_239', 'feature_411', 'feature_287', 'feature_295', 'feature_197', 'feature_211', 'feature_557', 'feature_319', 'feature_432', 'feature_449', 'feature_148', 'feature_5', 'feature_561', 'feature_204', 'feature_542', 'feature_141', 'feature_143', 'feature_428', 'feature_209', 'feature_340', 'feature_205', 'feature_284', 'feature_139', 'feature_160', 'feature_174', 'feature_308', 'feature_206', 'feature_368', 'feature_389', 'feature_479', 'feature_144', 'feature_164', 'feature_437', 'feature_312', 'feature_453', 'feature_470', 'feature_18', 'feature_118', 'feature_54', 'feature_250', 'feature_229', 'feature_447', 'feature_172', 'feature_348', 'feature_456', 'feature_251', 'feature_168', 'feature_74', 'feature_351', 'feature_360', 'feature_472', 'feature_407', 'feature_37', 'feature_280', 'feature_249', 'feature_307', 'feature_311', 'feature_276', 'feature_125', 'feature_336', 'feature_426', 'feature_188', 'feature_154', 'feature_435', 'feature_208', 'feature_200', 'feature_589', 'feature_162', 'feature_343', 'feature_115', 'feature_554', 'feature_28', 'feature_283', 'feature_339', 'feature_186', 'feature_350', 'feature_210', 'feature_201', 'feature_455', 'feature_122', 'feature_99', 'feature_342', 'feature_27', 'feature_386', 'feature_65', 'feature_278', 'feature_68', 'feature_67', 'feature_215', 'feature_576', 'feature_97', 'feature_100', 'feature_562', 'feature_334', 'feature_310', 'feature_306', 'feature_272', 'feature_149', 'feature_526', 'feature_421', 'feature_301', 'feature_541', 'feature_494', 'feature_305', 'feature_366', 'feature_35', 'feature_157', 'feature_94', 'feature_299', 'feature_413', 'feature_570', 'feature_216', 'feature_525', 'feature_361', 'feature_458', 'feature_302', 'feature_151', 'feature_296', 'feature_550', 'feature_185', 'feature_273', 'feature_584', 'feature_291', 'feature_568', 'feature_410', 'feature_344', 'feature_555', 'feature_274', 'feature_392', 'feature_106', 'feature_61', 'feature_224', 'feature_31', 'feature_177', 'feature_585', 'feature_318', 'feature_574', 'feature_300', 'feature_440', 'feature_524', 'feature_281', 'feature_163', 'feature_288', 'feature_304', 'feature_393', 'feature_155', 'feature_337', 'feature_436', 'feature_468', 'feature_567', 'feature_573', 'feature_167', 'feature_322', 'feature_20', 'feature_170', 'feature_136', 'feature_189', 'feature_214', 'feature_292', 'feature_577', 'feature_498', 'feature_46', 'feature_196', 'feature_367', 'feature_569', 'feature_446', 'feature_358', 'feature_390', 'feature_147', 'feature_199', 'feature_333', 'feature_394', 'feature_279', 'feature_165', 'feature_253', 'feature_178', 'feature_471', 'feature_17', 'feature_317', 'feature_558', 'feature_256', 'feature_153', 'feature_173', 'feature_152', 'feature_362', 'feature_156', 'feature_145', 'feature_363', 'feature_408', 'feature_335', 'feature_309', 'feature_442', 'feature_495', 'feature_454', 'feature_474', 'feature_120', 'feature_202', 'feature_480', 'feature_551', 'feature_198', 'feature_422', 'feature_520', 'feature_218', 'feature_219', 'feature_220', 'feature_175', 'feature_460', 'feature_123', 'feature_207', 'feature_427', 'feature_497', 'feature_341', 'feature_346', 'feature_217', 'feature_457', 'feature_544', 'feature_553', 'feature_240', 'feature_540', 'feature_184', 'feature_140', 'feature_203', 'feature_226', 'feature_406', 'feature_47', 'feature_431', 'feature_303', 'feature_255', 'feature_409']\n",
      "\n",
      "Initial columns: 452\n",
      "Final columns: 220\n"
     ]
    }
   ],
   "source": [
    "# High Correlation columns\n",
    "# Initial column count\n",
    "# initial_cols = train_df_clean.shape[1]\n",
    "\n",
    "# #  Identify feature columns (exclude target and date)\n",
    "# feature_cols = [col for col in train_df_clean.columns if col not in ['is_faulty', 'date_time']]\n",
    "\n",
    "# # Compute correlation matrix and find highly correlated columns to drop\n",
    "# corr_matrix = train_df_clean[feature_cols].corr().abs()\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# threshold = 0.8\n",
    "# high_corr_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "# # train_df_clean = train_df_clean.drop(columns=high_corr_drop)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Columns dropped due to high correlation: {len(high_corr_drop)}\")\n",
    "# print(\"\\nList of dropped columns:\")\n",
    "# print(high_corr_drop)\n",
    "\n",
    "# # Final count\n",
    "# final_cols = train_df_clean.shape[1]\n",
    "# print(f\"\\nInitial columns: {initial_cols}\")\n",
    "# print(f\"Final columns: {final_cols}\")\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initial column count\n",
    "initial_cols = train_df_clean.shape[1]\n",
    "\n",
    "# Identify feature columns (exclude target and date)\n",
    "feature_cols = [col for col in train_df_clean.columns if col not in ['is_faulty', 'date_time']]\n",
    "\n",
    "# Ensure all features are numeric \n",
    "X = train_df_clean[feature_cols]\n",
    "if X.select_dtypes(include=['object', 'category']).shape[1] > 0:\n",
    "    X = pd.get_dummies(X)\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Compute MI scores\n",
    "y = train_df_clean['is_faulty'].values\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "mi_series = pd.Series(mi_scores, index=X.columns)\n",
    "\n",
    "# Compute correlation matrix and its upper triangle\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "threshold = 0.8\n",
    "to_drop = set()\n",
    "\n",
    "# For each pair of highly correlated columns, drop the one with lower MI\n",
    "for col in upper.columns:\n",
    "    for row in upper.index:\n",
    "        if pd.notnull(upper.loc[row, col]) and upper.loc[row, col] > threshold:\n",
    "            if mi_series[col] < mi_series[row]:\n",
    "                to_drop.add(col)\n",
    "            else:\n",
    "                to_drop.add(row)\n",
    "\n",
    "# Drop the selected columns\n",
    "train_df_clean = train_df_clean.drop(columns=list(to_drop))\n",
    "\n",
    "# Print results\n",
    "print(f\"Columns dropped due to high correlation: {len(to_drop)}\")\n",
    "print(\"\\nList of dropped columns:\")\n",
    "print(list(to_drop))\n",
    "\n",
    "# Final count\n",
    "final_cols = train_df_clean.shape[1]\n",
    "print(f\"\\nInitial columns: {initial_cols}\")\n",
    "print(f\"Final columns: {final_cols}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2080f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing outliers (3-sigma rule):\n",
      "['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_15', 'feature_16', 'feature_19', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_30', 'feature_33', 'feature_34', 'feature_36', 'feature_38', 'feature_39', 'feature_40', 'feature_42', 'feature_44', 'feature_45', 'feature_49', 'feature_51', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_62', 'feature_63', 'feature_64', 'feature_66', 'feature_69', 'feature_71', 'feature_72', 'feature_73', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_95', 'feature_96', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_107', 'feature_108', 'feature_109', 'feature_113', 'feature_114', 'feature_116', 'feature_117', 'feature_119', 'feature_121', 'feature_124', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_131', 'feature_132', 'feature_134', 'feature_135', 'feature_138', 'feature_146', 'feature_161', 'feature_166', 'feature_169', 'feature_171', 'feature_176', 'feature_181', 'feature_182', 'feature_183', 'feature_212', 'feature_213', 'feature_222', 'feature_223', 'feature_225', 'feature_228', 'feature_248', 'feature_252', 'feature_254', 'feature_268', 'feature_270', 'feature_271', 'feature_275', 'feature_282', 'feature_286', 'feature_289', 'feature_290', 'feature_297', 'feature_298', 'feature_313', 'feature_320', 'feature_321', 'feature_324', 'feature_325', 'feature_332', 'feature_338', 'feature_345', 'feature_347', 'feature_349', 'feature_352', 'feature_353', 'feature_354', 'feature_355', 'feature_356', 'feature_364', 'feature_369', 'feature_377', 'feature_378', 'feature_388', 'feature_391', 'feature_412', 'feature_414', 'feature_416', 'feature_417', 'feature_418', 'feature_424', 'feature_425', 'feature_429', 'feature_430', 'feature_433', 'feature_434', 'feature_438', 'feature_439', 'feature_441', 'feature_443', 'feature_445', 'feature_461', 'feature_469', 'feature_473', 'feature_475', 'feature_476', 'feature_477', 'feature_478', 'feature_481', 'feature_484', 'feature_485', 'feature_486', 'feature_488', 'feature_490', 'feature_491', 'feature_492', 'feature_496', 'feature_511', 'feature_521', 'feature_522', 'feature_523', 'feature_527', 'feature_528', 'feature_545', 'feature_546', 'feature_547', 'feature_548', 'feature_552', 'feature_556', 'feature_559', 'feature_560', 'feature_563', 'feature_564', 'feature_565', 'feature_566', 'feature_571', 'feature_572', 'feature_578', 'feature_583', 'feature_586', 'feature_587', 'feature_588', 'feature_590']\n",
      "count:\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Select numeric columns, excluding target and date if needed\n",
    "numeric_cols = train_df_clean.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['is_faulty', 'date_time']]\n",
    "\n",
    "outlier_columns = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    data = train_df_clean[col]\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    lower = mean - 3 * std\n",
    "    upper = mean + 3 * std\n",
    "    outliers = (data < lower) | (data > upper)\n",
    "    if outliers.any():\n",
    "        outlier_columns.append(col)\n",
    "\n",
    "print(\"Columns containing outliers (3-sigma rule):\")\n",
    "print(outlier_columns)\n",
    "print(\"count:\")\n",
    "print(len(outlier_columns))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
